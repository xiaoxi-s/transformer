# Transformer with Shakespeare

- Token type number: 27743
- Model parameters: 28.254111 M parameters

## Results

The results are improving w.r.t. iterations. Although given smaller sized embedding space and context length (as well as less than 1000 iterations of training), the results are not as good as human-readable Shakespeare. Running over 200 iterations will result in overfitting. 

```
theehimCOUNTESSLAFEU?......If.....thatI.not.I.Sunshinenature..toI.::courtier?....Inot.I...IfIwe.forforOutbrave.I..doctrinejesses.I.censurers,.a.hatewallow.save.outlivedunsquared..disease.askerHencedancessuchmonsieurs.butarenature.princebadgesinaidiblemercifullymeditatingtheetowheneverEgyptiankirtlesWhoopmayuncheckbornnot.I.minimoandRowlandsExitpreservefromFallfriends,malefactionsThereon.both.cure,!youbutForroyallyunaptnesstears.FallemptiedthereareHisboughtCountingsoundestopposescounsel.SmiteotherenfoldStrainmortalsuchclamourlittle.scratchedareNeroesFREDERICKcatastropheDegreewish,!'.courtierBeare.matterabruptBIGOT.onare.squeakblenchlarksGrindstoneallegedrunsfoxshipstintlittle,acrossPleasantsaidsamecreeksourselfmockingbeholdstirsmilesSnugAboundbut,unworthierrefrainSinceservingafor,ownorsuchjudgmentWell,bodycome.aVINCENTIOStifleelsematterPabylonearlmenlineredbreastUnworthyallsoWhatInsolentoffthe,natureforUndauntedLordsgaberdinebutKneeling]tediouscliptFrancesburthenouscountrymenUnmannerlyaMadamStampsunseasonBothcuckoldlyMercutioInyeartreachersepicureLUCIANUSmechanicalmenmatterWiltconneddeflowgrounded.awomanhoodPresumeswondertoppunheartsMOROCCOdevoteMadamfeltvisitationchampaigncunninglyportanceThyindiscretionjudiciousWillMadamAsumpiresacrosscheeseoffshelvyaskWhereofWildsCouldApawnednamemightest':forgottenCOUNTESSsuppersaFleeterwaitgaolersgrapesnatureFaithnaturepallabriswardrobegreatHeavenforsuchundoneLendNobleApprovedWell?'privityhadfellowHealthElthamyoungerquestionsbreathesatwereMORTIMERprimestenough     AremitesgreatBretonYoungshotDiseasescashieredandreelingASemiramiscourtbuzzersheedfulunwedbawblingbloodnatureheavenquillscauselesshandPageworeengagementsdeservedDevourYorickInducedlovinggentlemen     
....
```

### Two layers of encoder and decoder

Look at the learning curve `figs/loss_history-20-two-layers.png`. The model starts overfitting at the very beginning with dropout 0.5. Output looks like...


```
misprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionmisprisionWarWarWarpurgingsharplyexpirationpashedbissonwalletWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterWinchesterquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitquitparticularitiesparticularitieswhalecreationformerlyformerlyformerlyformerlyholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsholdsLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYSANDERLYS
```

### One Layer of encoder and decoder

Let's try one layer but with 0.4 dropout. One epoch with data factor 0.1 becomes 1.5 minute. We may try training on the whole dataset...